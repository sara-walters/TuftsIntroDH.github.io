{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: NLP\n",
    "subtitle: \"Natural Language Processing and Named Entity Recognition\"\n",
    "author:\n",
    "  - name: Charles Pletcher\n",
    "    affiliations: Tufts University\n",
    "    orcid: 0000-0003-2734-5511\n",
    "    email: charles.pletcher@tufts.edu\n",
    "license:\n",
    "  code: MIT\n",
    "date: 2025-04-06\n",
    "---\n",
    "\n",
    "# Natural Language Processing\n",
    "\n",
    "NLP is a large field — we'll only be able to scratch the surface today. We've already started working with elements of NLP, however: tokenization is a common first step in NLP, and it is also an NLP problem in itself.\n",
    "\n",
    "As we've discussed, tokenization is not as simple as breaking on whitespace, nor even as simple as breaking on whitespace and punctuation. How should we handle hyphenated words, for example? What about \"U.K.\" or \"U.S.\"?\n",
    "\n",
    "## Named Entity Recognition\n",
    "\n",
    "Another subtask of NLP is Named Entity Recognition (NER). NER itself is composed of other sub-problems like named entity classification (\"What kind of entity is this?\") and named entity linking (\"To what specific entity does this refer?\")\n",
    "\n",
    "Today, we'll be focusing on named entity classification of Book 1 of Pausanias' _Periegesis_. We'll be able to look up these entities in a data dump from the Pleiades project and feed them back into ArcGIS along with their coordinates and other relevant information.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "First, let's read in the transcription of Book 1 that we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paus_filename = Path(\"./txt/tlg0525.tlg001.theoi-eng.txt\")\n",
    "\n",
    "with open(paus_filename) as f:\n",
    "    book_1 = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple enough. Let's just peek at the data to make sure it looks sane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_1[100:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty reasonable to me!\n",
    "\n",
    "## Installing spaCy\n",
    "\n",
    "[spaCy](https://spacy.io) is a Python library for NLP. Unlike the [NLTK](https://nltk.org), which prioritizes teaching and research, spaCy generally provides one way of performing a given task. For our purposes, spaCy's guided approach will be more than sufficient.\n",
    "\n",
    "To get started, install spaCy like any other Python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for spaCy to perform anything of use, we also need to download a pretrained _model_. Models are essentially large mappings of tokens (or subtokens) to long matrixes (lists of lists) of numbers. The larger the model, the more accurately it can represent a text in numerical terms — but also the more expensive it is to run.\n",
    "\n",
    "We'll use the medium model today, as it hits the sweet spot for accuracy and usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model downloaded, we can now run the text of `book_1` through spaCy's NER pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the model that we downloaded.\n",
    "# If this line fails, make sure that\n",
    "# you have downloaded the model that's\n",
    "# referenced here.\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Analyze `book_1` — this might take a bit.\n",
    "doc = nlp(book_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're running these lines in a separate cell so that we don't\n",
    "# need to run the full analysis each time we inspect the results.\n",
    "\n",
    "ents = [(e.text, e.label_) for e in doc.ents]\n",
    "\n",
    "for ent in ents:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "What is the type of the results in `ents`?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking up coordinates\n",
    "\n",
    "While these results are far from perfect — \"Hyllus,\" at least in my practice runs, was classified as a \"PRODUCT\" rather than a \"PERSON\" — they're fairly useful in broad strokes for our purposes.\n",
    "\n",
    "But we still need to add coordinates, and we have over 4000 entities to link. How can we go about doing this scalably?\n",
    "\n",
    "## Build a search tool\n",
    "\n",
    "All of the data we need is available through [Pleiades](https://pleiades.stoa.org) and [ToposText](https://topostext.org), but the strings that are labeled by our NER model might not match the titles of places available from these sources. We could build a search index that lets us match titles mor flexibly, but that is beyond the scope of our work for today.\n",
    "\n",
    "## Annotate by hand\n",
    "\n",
    "Instead, working in groups, choose about **20** places from the NER list that you would like to map. You could even pull them out randomly, if you'd like.\n",
    "\n",
    "Then, using Pleiades's own search tool, find the coordinates for each location. Store this data, along with any contextual information or descriptions that you deem relevant, in a CSV or spreadsheet that you can upload to ArcGIS.\n",
    "\n",
    ":::{note}\n",
    "Can you also include a `count` parameter for how often each place is mentioned in Book 1?\n",
    ":::\n",
    "\n",
    "If you find that your group is working particularly quickly, grab another 10 placenames, or experiment with mapping specific sections of Pausanias' text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "- <https://doi.org/10.5334/johd.150>\n",
    "- <https://doi.org/10.5281/zenodo.1193921>\n",
    "- Kirsch, Adam. \"Technology Is Taking Over English Departments.\" The New Republic, May 2, 2014\n",
    "- @Blei2012\n",
    "- @Brett2012\n",
    "- @Mimno2012\n",
    "- @Wellmon2015\n",
    "\n",
    "## Homework\n",
    "\n",
    "- Finish annotating placenames and uploading the results to an ArcGIS map\n",
    "- Share a link to the map on Canvas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
